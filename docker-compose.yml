# Docker Compose for Movie Recommendation System with Airflow
# Run: docker-compose -f docker-compose.airflow.yml up -d

version: "3.8"

x-airflow-common: &airflow-common
  image: apache/airflow:2.8.0-python3.11
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
    - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=false
    - AIRFLOW__CORE__LOAD_EXAMPLES=false
    - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=true
    - KAGGLE_USERNAME=${KAGGLE_USERNAME}
    - KAGGLE_KEY=${KAGGLE_KEY}
    - TMDB_API_KEY=${TMDB_API_KEY:-44726ef95f4d79cb7001a4947fca7f53}
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./:/opt/airflow/movie-rec
    - airflow-kaggle:/root/.kaggle
  depends_on:
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres-db:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "airflow" ]
      interval: 5s
      retries: 5
    restart: unless-stopped

  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        pip install kaggle faiss-cpu scikit-learn pandas pyarrow pandera joblib
        airflow db init
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true
        mkdir -p /root/.kaggle
        echo '{"username":"'$$KAGGLE_USERNAME'","key":"'$$KAGGLE_KEY'"}' > /root/.kaggle/kaggle.json
        chmod 600 /root/.kaggle/kaggle.json
    restart: "no"

  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    command: bash -c "pip install kaggle faiss-cpu scikit-learn pandas pyarrow pandera joblib && airflow webserver"
    ports:
      - "8080:8080"
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8080/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: bash -c "pip install kaggle faiss-cpu scikit-learn pandas pyarrow pandera joblib && airflow scheduler"
    healthcheck:
      test: [ "CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname)" ]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Movie Recommendation Backend
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: movie-rec-backend
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
    environment:
      - TMDB_API_KEY=${TMDB_API_KEY:-44726ef95f4d79cb7001a4947fca7f53}
    command: uvicorn backend.main:app --host 0.0.0.0 --port 8000
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Movie Recommendation Frontend
  frontend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: movie-rec-frontend
    ports:
      - "8501:8501"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
    environment:
      - API_BASE_URL=http://backend:8000
    command: streamlit run app.py --server.port 8501 --server.address 0.0.0.0
    depends_on:
      - backend
    restart: unless-stopped

volumes:
  postgres-db:
  airflow-kaggle:
